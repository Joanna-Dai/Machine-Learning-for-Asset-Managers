{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afdf053b",
   "metadata": {},
   "source": [
    "[Chapter 3] Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "9d0a655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "import scipy.stats as ss\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92eb377",
   "metadata": {},
   "source": [
    "generate random variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "5adc32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# generate linearly correlated random variables\n",
    "x = np.random.randn(1000)\n",
    "y = 0.5 * x + 0.1 * np.random.randn(1000)  # add some noise\n",
    "bins = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "9f8f75c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  8.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  4.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   9.,  56.,   5.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,  18., 110.,  18.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,  31., 162.,  24.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,  34., 171.,  34.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   1.,  32., 112.,  15.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,  16.,  50.,  20.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   7.,  22.,   9.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   2.,  13.]])"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# discretized into regular grid with a number of partition (bins) per dimension\n",
    "\n",
    "# joint counts/distribution\n",
    "cXY = np.histogram2d(x, y, bins)[0] \n",
    "print(np.sum(cXY))\n",
    "cXY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16c61e",
   "metadata": {},
   "source": [
    "marginal entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "1a15b4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9528011589708985\n",
      "1.9528011579708984\n"
     ]
    }
   ],
   "source": [
    "# entropy of X: the expected value of surprises, the amount of uncertainty associated with X\n",
    "\n",
    "# hX = -sum{p(x)*log(p(x))} \n",
    "# log(p(x)) measures hows surprising as surprising are characterized by low prob\n",
    "hX = ss.entropy(np.histogram(x, bins)[0])\n",
    "print(hX)\n",
    "# bins = 1, hX = 0 (zero entropy when all prob is concerntrated in a single element)\n",
    "# bins = 10, hX = 2.301 (entropy closer to max when X is distributed more uniformly)\n",
    "\n",
    "# check with alternative\n",
    "cX = np.histogram(x, bins)[0] # count\n",
    "pX = cX/np.sum(cX)            # prob\n",
    "hX_alt = -np.sum(pX * np.log(pX + 1e-10))\n",
    "print(hX_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "03e1174d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9943498307707825\n"
     ]
    }
   ],
   "source": [
    "# entropy of Y\n",
    "hY = ss.entropy(np.histogram(y, bins)[0])\n",
    "print(hY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3f6ad",
   "metadata": {},
   "source": [
    "mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "4bb604ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2222238074758958\n",
      "0.6258823648588945\n"
     ]
    }
   ],
   "source": [
    "# decrease in uncertainty (or information gain) in X that results from knowing the value Y\n",
    "\n",
    "# iXY = hX - hX_Y = hX + hY - hXY\n",
    "# iXX = hX\n",
    "# iXY = 0 (when X are Y are independent) x = 0\n",
    "# iXY <= min{hX, hY}\n",
    "\n",
    "iXY = mutual_info_score(None, None, contingency=cXY) # when y = 0*x + 1*e, iXY = 0.048 -> 0\n",
    "print(iXY)\n",
    "# normalized mutual information\n",
    "iXYn = iXY/min(hX, hY)\n",
    "print(iXYn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c7ab7b",
   "metadata": {},
   "source": [
    "joint entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "eca3bb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7249271822657857"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hXY = hX + hY - iXY\n",
    "hXY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c60570",
   "metadata": {},
   "source": [
    "conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "5e43f291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7305773514950031, 0.7721260232948872)"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the conditional entropy of X given Y\n",
    "hX_Y = hXY - hY\n",
    "# the conditoinal entropy of Y given X\n",
    "hY_X = hXY - hX\n",
    "\n",
    "hX_Y, hY_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458882f",
   "metadata": {},
   "source": [
    "variation of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "2340386b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5027033747898897\n",
      "1.50270337478989 1.5027033747898904\n"
     ]
    }
   ],
   "source": [
    "# vXY = hXY - iXY = 2*hXY - hX - hY = hX + hY - 2*iXY = hX_Y + hY_X\n",
    "# the uncertainty we expect in one variable if we are told the value of the other\n",
    "\n",
    "def varInfo(x, y, bins, norm=False):\n",
    "    \"\"\"\n",
    "    variation of information\n",
    "    \"\"\"\n",
    "    cXY = np.histogram2d(x, y, bins)[0]\n",
    "    iXY = mutual_info_score(None, None, contingency=cXY)\n",
    "    hX = ss.entropy(np.histogram(x, bins)[0])\n",
    "    hY = ss.entropy(np.histogram(y, bins)[0])\n",
    "    vXY = hX + hY - 2*iXY\n",
    "    if norm:\n",
    "        hXY = hX + hY -iXY  # joint entropy\n",
    "        vXY /= hXY # normalized variation information\n",
    "    return vXY\n",
    "\n",
    "vXY = varInfo(x, y, bins)\n",
    "print(vXY)\n",
    "# check\n",
    "print(hXY-iXY, hX_Y + hY_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513a7e4",
   "metadata": {},
   "source": [
    "on discretized continuous random variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660a95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numBins(nObs, corr=None):\n",
    "    \"\"\"\n",
    "    optimal number of bins for discretization\n",
    "    \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
