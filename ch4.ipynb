{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbead2e7",
   "metadata": {},
   "source": [
    "[Chapter 4] Optimal Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a clustering problem consists of a set of objects and a set of features associated with those objects (unsupervised learning)\n",
    "# the goal is to seperate the objects into groups (called clusters) using the features\n",
    "#           where intragroup similarities are maximized and intergroup similarities are minimized\n",
    "\n",
    "# 2 main classes of clustering: partitional (one-level/un-nested) and hierarchical(nested sequence of partitions)\n",
    "# types of clustering algo: connectivity (e.g. hierarchical clustering), centroids (e.g. k-means), distribution,\n",
    "#                      density (e.g. DBSCAN, OPTICS), subspace (on 2-dimensions both features and observations, e.g. bi/co-clustering) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec2116",
   "metadata": {},
   "source": [
    "base clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aca6583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterKMeansBase(corr0, maxNumClusters=10, n_init=10):\n",
    "    \"\"\"\n",
    "    use k-means algorithm on observation matrix\n",
    "    \"\"\"\n",
    "    # derive observation matrix x as distance matrix\n",
    "    x = ((1-corr0.fillna(0))/2.)**.5    # corr=1, x=0; corr=-1, x=1\n",
    "    silh = pd.Series()\n",
    "    \n",
    "    for init in range(n_init):  # k-means uses multiple random initialization to avoid local optimas\n",
    "        for i in range(2, maxNumClusters+1): # loop different number of maxNumClusters\n",
    "            \n",
    "            # k-means clustering\n",
    "            kmeans_ = KMeans(n_clusters=i, n_jobs=1, n_init=1)  # only initialization once for inner loop\n",
    "            kmeans_ = kmeans_.fit(x)\n",
    "\n",
    "            #  calculate silhouette coef in measuring comparing intracluster distance and intercluster distance\n",
    "            #       Si = (b_i-a_i)/max{a_i, b_i} \n",
    "            #       - a_i is the avg distance between i and all other elements in the same cluster\n",
    "            #       - b_i is the avg distance between i and all the elements in the nearest cluster which i is not a member \n",
    "            #       - Si = 1 means i was clustered well and -1 means i was clustered poorly\n",
    "            silh_ = silhouette_samples(x, kmeans_.labels_)\n",
    "            \n",
    "            # clustering quality q = mean(silh)/std(silh)\n",
    "            # comparing current q_ vs historical optimal q\n",
    "            stat = (silh_.mean()/silh_.std(), silh.mean()/silh.std())\n",
    "            if np.isnan(stat[1]) or stat[0]>stat[1]:    \n",
    "                silh, kmeans=silh_, kmeans_     # select clustering with the highest q\n",
    "\n",
    "    # reorder correlation matrix based on kmeans clustering\n",
    "    newIdx = np.argsort(kmeans.labels_)\n",
    "    corr1 = corr0.iloc[newIdx] # reorder rows\n",
    "    corr1 = corr1.iloc[:, newIdx] # reorder columns\n",
    "\n",
    "    # extract/output clustering info\n",
    "    clstrs = {i:corr0.columns[np.where(kmeans.labels_==i)[0]].tolist() \\\n",
    "              for i in np.unique(kmeans.labels_)}   # cluster members\n",
    "    \n",
    "    silh = pd.Series(silh, index=x.index)\n",
    "    \n",
    "    return corr1, clstrs, silh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33227a",
   "metadata": {},
   "source": [
    "higher-level clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10e1371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNewOutputs(corr0, clstrs, clstrs2):\n",
    "    \"\"\"\n",
    "    combine two sets of clusters into one unified clustering result.\n",
    "    update the correlation matrix and silhouette scores accordingly.\n",
    "    \"\"\"\n",
    "    # take two dictionaries of clusters and merge these groups into a single new collection of clusters\n",
    "    clstrsNew = {}\n",
    "    for i in clstrs.keys():\n",
    "        clstrsNew[len(clstrsNew.keys())] = list(clstrs[i])\n",
    "    for i in clstrs2.keys():\n",
    "        clstrsNew[len(clstrsNew.keys())] = list(clstrs2[i])\n",
    "    \n",
    "    # get new idx of clstrsNew (outer loop i to loop each keys in cltrsNew, inner loop to loop each item for each key) \n",
    "    newIdx = [j for i in clstrsNew for j in clstrsNew[i]]\n",
    "    # extract the relevant subset of the original correlation matrix that corresponds to all items in the new combined clusters\n",
    "    corrNew = corr0.loc[newIdx, newIdx]\n",
    "    \n",
    "    # convert the correlation matrix into distance matrix \n",
    "    x = ((1-corr0.fillna(0))/2.)**.5\n",
    "    # assign cluster labels to each item in this new combined set\n",
    "    kmeans_labels = np.zeros(len(x.columns))\n",
    "    for i in clstrsNew.keys():\n",
    "        idxs = [x.index.get_loc(k) for k in clstrsNew[i]]\n",
    "        kmeans_labels[idxs] = i\n",
    "    # calculate silhouette scores, which measure how well each item fits within its cluster.\n",
    "    silhNew = pd.Series(silhouette_samples(x, kmeans_labels), index=x.index)\n",
    "    # returns the updated correlation matrix, combined clusters, and silhouette scores\n",
    "    return corrNew, clstrsNew, silhNew\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "\n",
    "def clusterKMeansTop(corr0, maxNumClusters=None, n_init=10):\n",
    "    \"\"\"\n",
    "    perform K-means clustering on the correlation matrix.\n",
    "    identify weak clusters based on silhouette scores.\n",
    "    recursively refine those weak clusters by re-clustering them.\n",
    "    decide whether the refinement improves clustering quality or not.\n",
    "    \"\"\"\n",
    "    # intial clustering\n",
    "    # run base K-means clustering on full dataset and get initial clusters and silh scores\n",
    "    if maxNumClusters==None: maxNumClusters=corr0.shape[1]-1\n",
    "    corr1, clstrs, silh=clusterKMeansBase(corr0, maxNumClusters=\\\n",
    "        min(maxNumClusters, corr0.shape[1]-1), n_init=n_init)\n",
    "    \n",
    "    # evaluate clusters\n",
    "    # calculate qualtiy(t-stat like) score for each cluster\n",
    "    clusterTstats = {i:np.mean(silh(clstrs[i])) / \\\n",
    "        np.std(silh[clstrs[i]]) for i in clstrs.keys()}\n",
    "    # compute the average quality across all clusters\n",
    "    tStatMean = sum(clusterTstats.values())/len(clusterTstats)\n",
    "    \n",
    "    # identify weak clusters\n",
    "    # find the set of weak clusters with quality below average\n",
    "    redoClusters = [i for i in clusterTstats.keys() if \\\n",
    "        clusterTstats[i]<tStatMean]\n",
    "    # refine the weak clusters (redoClusters)\n",
    "    if len(redoClusters)<=1:\n",
    "        return corr1, clstrs, silh\n",
    "    else:\n",
    "        # if multiple weak clusters\n",
    "        # extract items in those weak clusters\n",
    "        keysRedo = [j for i in redoClusters for j in clstrs[i]]\n",
    "        corrTmp = corr0.loc[keysRedo, keysRedo]\n",
    "        tStatMean = np.mean([clusterTstats[i] for i in redoClusters])\n",
    "        # run the clustering function recursively on just those items, aiming to split or reorganize them into better clusters.\n",
    "        corr2, clstrs2, silh2 = clusterKMeansTop(corrTmp, \\\n",
    "            maxNumClusters=min(maxNumClusters, corrTmp.shape[1]-1), n_init=n_init)\n",
    "        # make new outputs\n",
    "        # merge the strong clusters (clstrs no in redoClusters) with the refined weak clusters (clstrs2)\n",
    "        corrNew, clstrsNew, silhNew = makeNewOutputs(corr0, \\\n",
    "            {i: clstrs[i] for i in clstrs.keys() if i not in redoClusters}, clstrs2)\n",
    "        # recalculate the evaluation metrics on this updated combined clustering\n",
    "        newTstatsMean = np.mean([np.mean(silhNew[clstrsNew[i]])/ \\\n",
    "            np.std(silhNew[clstrsNew[i]]) for i in clstrsNew.keys()])\n",
    "        # return the best clustering (either original or refined) \n",
    "        if newTstatsMean <= tStatMean:\n",
    "            return corr1, clstrs, silh   # otherwise, keep the original clustering\n",
    "        else:\n",
    "            return corrNew, clstrsNew, silhNew # if the new combined clustering is better, return the new ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033ff0a",
   "metadata": {},
   "source": [
    "experimental results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2caed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
